{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298d7157",
   "metadata": {},
   "source": [
    "# Model Activation Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f8418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushjain/Development/Interp/CoT-Faithfulness-Model-Diffing/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, json, argparse\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df86eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c42699",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3cd1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle_idx(hidden_len: int) -> int: \n",
    "    n_layers = hidden_len - 1\n",
    "    return 1 + (n_layers // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd5ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(model, input_ids, attention_mask) -> torch.Tensor:\n",
    "    \"\"\"Run one forward pass and return hidden_states[idx]: [B, S, D].\"\"\"\n",
    "    out = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True,\n",
    "        use_cache=False,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    return out.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be66b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_pair_single_layer(\n",
    "    hsA: torch.Tensor,\n",
    "    hsB: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    drop_bos: bool,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Align, drop BOS, mask padding, and return [N, 2, d_in] for ONE layer.\n",
    "\n",
    "    - Single tokenizer → SAME ids/mask sent to both models → token t aligns in A and B.\n",
    "    - Drop BOS (t=0) to avoid degenerate no-context token.\n",
    "    - Mask padding (attention_mask == 0) to keep only real tokens.\n",
    "    \"\"\"\n",
    "    if drop_bos:\n",
    "        hsA = hsA[:, 1:, :]\n",
    "        hsB = hsB[:, 1:, :]\n",
    "        mask = attention_mask[:, 1:]\n",
    "    else:\n",
    "        mask = attention_mask\n",
    "\n",
    "    valid = mask.bool().view(-1)           # [B*S’]\n",
    "    A = hsA.reshape(-1, hsA.size(-1))[valid]\n",
    "    B = hsB.reshape(-1, hsB.size(-1))[valid]\n",
    "    x = torch.stack([A, B], dim=1)         # [N, 2, d_in]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65137e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_shard(dir_path: Path, shard_id: int, x_cpu_np: np.ndarray, meta: dict):\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    path = dir_path / f\"acts_{shard_id:05d}.pt\"\n",
    "    torch.save({\"x\": torch.from_numpy(x_cpu_np), \"meta\": meta}, path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3251cdbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unsloth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_infer_setup\u001b[39m(model):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Safer inference defaults\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def _post_infer_setup(model):\n",
    "    # Safer inference defaults\n",
    "    try: model.gradient_checkpointing_disable()\n",
    "    except Exception: pass\n",
    "    try: model.config.gradient_checkpointing = False\n",
    "    except Exception: pass\n",
    "    try: model.config.use_cache = True\n",
    "    except Exception: pass\n",
    "    return model.eval()\n",
    "\n",
    "def load_unsloth_pair(\n",
    "    base_model: str = \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    adapter_dir: str = \"outputs/adapter\",\n",
    "    device_map: str = \"auto\",\n",
    "    load_in_4bit: bool = True,\n",
    "    max_seq_length: int = 4096,\n",
    "    dtype: Optional[str] = None,   # None lets Unsloth pick (good for 4-bit)\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns: (model_A, model_B, tokenizer)\n",
    "      - model_A: base (no LoRA)\n",
    "      - model_B: base + LoRA (loaded from adapter_dir)\n",
    "      - tokenizer: *single* tokenizer from base (used for both)\n",
    "    \"\"\"\n",
    "    # 1) One tokenizer (from BASE) to guarantee identical tokenization A vs B\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 2) Load base with Unsloth\n",
    "    model_A, _tokA = FastLanguageModel.from_pretrained(\n",
    "        model_name     = base_model,\n",
    "        load_in_4bit   = load_in_4bit,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype          = dtype,\n",
    "        device_map     = device_map,\n",
    "    )\n",
    "    model_A = _post_infer_setup(model_A)\n",
    "\n",
    "    # 3) Load LoRA adapter with Unsloth (from adapter dir). Unsloth will resolve base.\n",
    "    #    (This is supported: pointing to the adapter folder is enough.)  # refs in sources\n",
    "    model_B, _tokB = FastLanguageModel.from_pretrained(\n",
    "        model_name     = adapter_dir,\n",
    "        load_in_4bit   = load_in_4bit,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype          = dtype,\n",
    "        device_map     = device_map,\n",
    "    )\n",
    "    model_B = _post_infer_setup(model_B)\n",
    "\n",
    "    # 4) Optional sanity checks to catch accidental tokenizer drift\n",
    "    #    (we *still* force using `tokenizer` from base everywhere)\n",
    "    try:\n",
    "        assert _tokB.get_vocab() == tokenizer.get_vocab()\n",
    "    except Exception:\n",
    "        # If not equal, we still use `tokenizer` consistently for both models.\n",
    "        # This keeps activations aligned.\n",
    "        pass\n",
    "\n",
    "    return model_A, model_B, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ceb56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'Qwen/Qwen2.5-7B-Instruct', 'adapter': 'adapter', 'dataset': {'name': 'opeani/gsm8k', 'subset': 'main', 'split': 'train', 'field': 'question'}, 'seq_len': 2048, 'batch_size': 8, 'drop_bos': True, 'dtype': 'bf16', 'out_dir': 'activations', 'seed': 42}\n",
      "{'name': 'opeani/gsm8k', 'subset': 'main', 'split': 'train', 'field': 'question'}\n"
     ]
    }
   ],
   "source": [
    "import yaml \n",
    "\n",
    "# Load from a YAML file\n",
    "with open(\"acts_config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(config)\n",
    "print(config['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the path for this data \n",
    "out_dir = Path(config['out_dir'], config['model'])\n",
    "out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c73d8a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_unsloth_pair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load in the tokenizer \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m base, tuned, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_unsloth_pair\u001b[49m(base_model\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], adapter_dir\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madapter\u001b[39m\u001b[38;5;124m'\u001b[39m], max_seq_len\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_unsloth_pair' is not defined"
     ]
    }
   ],
   "source": [
    "# load in the tokenizer \n",
    "base, tuned, tokenizer = load_unsloth_pair(base_model=config['model'], adapter_dir=config['adapter'], max_seq_len=config['seq_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516d8cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# need to get the hidden_size \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m probe \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello world\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[1;32m      4\u001b[0m     out \u001b[38;5;241m=\u001b[39m model_A(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprobe, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# need to get the hidden_size \n",
    "probe = tokenizer(\"hello world\", return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad(): \n",
    "    out = model_A(**probe, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "hidden_len = len(out.hidden_states)   # = n_layers + 1\n",
    "d_in = out.hidden_states[-1].size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856293d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m idx_middle \u001b[38;5;241m=\u001b[39m middle_idx(\u001b[43mhidden_len\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_len' is not defined"
     ]
    }
   ],
   "source": [
    "idx_middle = middle_idx(hidden_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53139a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "@dataclass\n",
    "class LayerSpec: \n",
    "    name: str\n",
    "    index: int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [LayerSpec(\"-3\", -3), LayerSpec(\"-2\", -2), LayerSpec(f\"{middle_idx}\", middle_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1497f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m manifest \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madapter\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubset\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubset\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdevice\u001b[49m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_bos\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mbool\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop_bos\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_in\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(d_in),\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m: {name: idx \u001b[38;5;28;01mfor\u001b[39;00m name, idx \u001b[38;5;129;01min\u001b[39;00m layer_specs},\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema_per_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx: [N, 2, d_in]; model axis: [base, base+LoRA]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "manifest = {\n",
    "    \"base_model\": config['model'],\n",
    "    \"adapter_dir\": config['adapter'],\n",
    "    \"dataset\": config['dataset']['name'],\n",
    "    \"subset\": config['dataset']['subset'],\n",
    "    \"split\": config['dataset']['split'],\n",
    "    \"field\": config['dataset']['field'],\n",
    "    \"seq_len\": config['seq_len'],\n",
    "    \"dtype\": config['dtype'],\n",
    "    \"device\": device,\n",
    "    \"drop_bos\": bool(config['drop_bos']),\n",
    "    \"d_in\": int(d_in),\n",
    "    \"layers\": {name: idx for name, idx in layer_specs},\n",
    "    \"schema_per_layer\": \"x: [N, 2, d_in]; model axis: [base, base+LoRA]\",\n",
    "}\n",
    "\n",
    "with open(out_dir / \"manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f63b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubset\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(config['dataset']['name'], config['dataset']['subset'], config['dataset']['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd0274",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mds\u001b[49m), config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m      2\u001b[0m     part \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(start, \u001b[38;5;28mmin\u001b[39m(start \u001b[38;5;241m+\u001b[39m CHUNK, \u001b[38;5;28mlen\u001b[39m(ds))))\n\u001b[1;32m      3\u001b[0m     texts: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m part[config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "for start in range(0, len(ds), config['chunk']):\n",
    "    part = ds.select(range(start, min(start + CHUNK, len(ds))))\n",
    "    texts: List[str] = part[config['dataset']['field']]\n",
    "\n",
    "    # iterate over each element in the chunk in batches \n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), args.batch_size), desc=f\"Chunk {start//CHUNK}\"):\n",
    "        micro = texts[i : i + args.batch_size]\n",
    "        if not micro:\n",
    "            continue\n",
    "\n",
    "        # Tokenize ONCE → SAME ids/mask for both models (alignment contract)\n",
    "        enc = tokenizer(\n",
    "            micro,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=args.seq_len,\n",
    "        ).to(args.device)\n",
    "\n",
    "        hsA = get_hidden_states(model_A, enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "        hsB = get_hidden_states(model_B, enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "\n",
    "        for obj in layers:\n",
    "            name = obj.name \n",
    "            idx = obj.index \n",
    "\n",
    "            x = flatten_pair_single_layer(hsA[idx], hsB[idx], enc['attention_mask', drop_bos=config['drop_bos']])\n",
    "            x_cpu = x.detach().to(\"cpu\")\n",
    "            accum[name].append(x_cpu)\n",
    "            rows_in_shard[name] += x_cpu.shape[0]\n",
    "\n",
    "            if rows_in_shard[name] >= args.shard_rows:\n",
    "                X = torch.cat(accum[name], dim=0).numpy()   # [M,2,D]\n",
    "                write_shard(layer_dirs[name], shard_ids[name], X, {\n",
    "                    **manifest, \"which_layer\": name, \"which_index\": idx\n",
    "                })\n",
    "                shard_ids[name] += 1\n",
    "                rows_in_shard[name] = 0\n",
    "                accum[name] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush remaining shards\n",
    "for name, idx in layer_specs:\n",
    "    if rows_in_shard[name] > 0 and len(accum[name]) > 0:\n",
    "        X = torch.cat(accum[name], dim=0).numpy()\n",
    "        write_shard(layer_dirs[name], shard_ids[name], X, {\n",
    "            **manifest, \"which_layer\": name, \"which_index\": idx\n",
    "        })\n",
    "\n",
    "print(f\"Done. Wrote activations to: {out_dir.resolve()}\")\n",
    "for name in layer_dirs:\n",
    "    print(f\"  Layer '{name}' dir: {layer_dirs[name]}\")\n",
    "print(\"Each shard has x with shape [N, 2, d_in] and meta indicating which layer.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

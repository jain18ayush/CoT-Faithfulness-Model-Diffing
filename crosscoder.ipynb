{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c3f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushjain/Development/Interp/CoT-Faithfulness-Model-Diffing/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b5360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple, Optional, Union\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pprint # For printing config nicely\n",
    "\n",
    "\n",
    "# Define available data types\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "\n",
    "\n",
    "# Define a structure for the loss output for clarity\n",
    "class LossOutput(NamedTuple):\n",
    "    l2_loss: torch.Tensor          # Reconstruction error\n",
    "    l1_loss: torch.Tensor          # Sparsity penalty\n",
    "    l0_loss: torch.Tensor          # Count of active features\n",
    "    explained_variance: torch.Tensor # Overall variance explained\n",
    "    explained_variance_A: torch.Tensor # Variance explained for Model A\n",
    "    explained_variance_B: torch.Tensor # Variance explained for Model B\n",
    "\n",
    "class CrossCoder(nn.Module):\n",
    "    def __init___(): \n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        d_hidden = self.cfg[\"dict_size\"]\n",
    "        self.device = self.cfg[\"device\"] # Store the device as an instance attribute\n",
    "        d_in = self.cfg[\"d_in\"]\n",
    "        n_models = 2\n",
    "        self.dtype = DTYPES[self.cfg[\"enc_dtype\"]]\n",
    "        torch.manual_seed(self.cfg[\"seed\"])\n",
    "\n",
    "\n",
    "        self.W_enc = nn.Parameter(torch.empty(n_models, d_in, d_hidden, dtype=self.dtype))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=self.dtype))\n",
    "        self.W_dec = nn.Parameter(torch.empty(d_hidden, n_models, d_in, dtype=self.dtype))\n",
    "        self.b_dec = nn.Parameter(torch.zeros((n_models, d_in), dtype=self.dtype))\n",
    "\n",
    "\n",
    "        nn.init.normal_(self.W_dec, std=1.0)\n",
    "        dec_norm = self.W_dec.norm(dim=-1, keepdim=True)\n",
    "        self.W_dec.data /= (dec_norm + 1e-8)\n",
    "        self.W_dec.data *= self.cfg[\"dec_init_norm\"]\n",
    "        self.W_enc.data = einops.rearrange(self.W_dec.data.clone(), \"h n d -> n d h\")\n",
    "\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.to(self.cfg[\"device\"])\n",
    "        self.save_dir = None\n",
    "        self.save_version = 0\n",
    "        print(f\"CrossCoder (BatchTopK variant) initialized on device: {self.cfg['device']}\")\n",
    "        if self.cfg.get(\"sparsity_type\", \"l1\") == \"batch_top_k\":\n",
    "            assert \"k_sparsity\" in self.cfg, \"k_sparsity must be in cfg for BatchTopK\"\n",
    "            print(f\"Using BatchTopK sparsity with k={self.cfg['k_sparsity']}\")\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, is_training: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes input activations. Applies BatchTopK if training and configured.\n",
    "        Otherwise, applies ReLU (for inference or L1 mode).\n",
    "        \"\"\"\n",
    "        pre_acts = self.get_pre_activations(x) # Shape: [batch, d_hidden]\n",
    "\n",
    "\n",
    "        if is_training and self.cfg.get(\"sparsity_type\", \"l1\") == \"batch_top_k\":\n",
    "\n",
    "\n",
    "            relu_acts = F.relu(pre_acts) # fj(xi)\n",
    "\n",
    "\n",
    "            with torch.no_grad(): # Decoder norms don't need gradients here\n",
    "                decoder_norms_sum = self.W_dec.norm(p=2, dim=-1).sum(dim=-1) # Shape [d_hidden]\n",
    "                                                                        # (||d_base_j|| + ||d_chat_j||)\n",
    "\n",
    "\n",
    "            v_values = relu_acts * decoder_norms_sum.unsqueeze(0) # Shape [batch, d_hidden]\n",
    "\n",
    "\n",
    "            k = self.cfg[\"k_sparsity\"]\n",
    "            num_inputs = x.shape[0] # batch_size\n",
    "            num_to_select_total = num_inputs * k # Total top activations across the batch\n",
    "\n",
    "\n",
    "            # Flatten v_values to find global top-k across batch and features\n",
    "            top_k_values, _ = torch.topk(v_values.flatten(), k=num_to_select_total)\n",
    "\n",
    "\n",
    "            if top_k_values.numel() == 0: # Handle case where no values are selected (e.g. k=0)\n",
    "                # If k=0 or no positive activations, all are zero\n",
    "                threshold = float('inf')\n",
    "            elif num_to_select_total == 0: # k=0\n",
    "                threshold = float('inf')\n",
    "            else:\n",
    "                # The threshold is the k-th largest value\n",
    "                threshold = top_k_values[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Create a mask for activations >= threshold\n",
    "            # Only keep activations if their v_value was among the top k*N\n",
    "            train_acts_mask = v_values >= threshold\n",
    "\n",
    "\n",
    "            # Apply the mask to the ReLU activations (not pre_acts directly for f_train)\n",
    "            # This ensures f_train (output here) matches the paper's f_j(x_i) for selected, 0 otherwise\n",
    "            final_acts = relu_acts * train_acts_mask.float()\n",
    "\n",
    "\n",
    "        else: # Standard ReLU for inference or if L1 sparsity is used\n",
    "            final_acts = F.relu(pre_acts)\n",
    "\n",
    "\n",
    "        return final_acts\n",
    "\n",
    "    def get_pre_activations(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Calculates pre-activations (before ReLU or TopK).\"\"\"\n",
    "        x_enc = einops.einsum(x, self.W_enc, \"b n d, n d h -> b h\")\n",
    "        pre_acts = x_enc + self.b_enc # Shape: [batch, d_hidden]\n",
    "        return pre_acts\n",
    "\n",
    "    def decode(self, acts: torch.Tensor) -> torch.Tensor:\n",
    "        # (decode function remains the same)\n",
    "        acts_dec = einops.einsum(acts, self.W_dec, \"b h, h n d -> b n d\")\n",
    "        x_reconstruct = acts_dec + self.b_dec\n",
    "        return x_reconstruct\n",
    "\n",
    "    def forward(self, x: torch.Tensor, is_training: bool = True) -> torch.Tensor:\n",
    "        # Pass is_training to encode\n",
    "        acts = self.encode(x, is_training=is_training)\n",
    "        x_reconstruct = self.decode(acts)\n",
    "        return x_reconstruct\n",
    "    \n",
    "    def get_losses(self, x: torch.Tensor, is_training: bool = True) -> LossOutput:\n",
    "        x = x.to(self.cfg[\"device\"]).to(self.dtype)\n",
    "\n",
    "\n",
    "        # --- Forward pass (use is_training for encode) ---\n",
    "        acts = self.encode(x, is_training=is_training)\n",
    "        x_reconstruct = self.decode(acts)\n",
    "\n",
    "\n",
    "        # --- L2 Loss (Mean Squared Error) ---\n",
    "        diff = (x_reconstruct - x).float()\n",
    "        squared_diff = diff.pow(2)\n",
    "        l2_per_batch = einops.reduce(squared_diff, 'b n d -> b', 'sum')\n",
    "        l2_loss = l2_per_batch.mean()\n",
    "\n",
    "\n",
    "        # --- Sparsity Related Losses ---\n",
    "        l1_loss_val = torch.tensor(0.0, device=self.device) # Default for BatchTopK\n",
    "        l0_loss_val = (acts > 1e-8).float().sum(dim=-1).mean() # L0 is always informative\n",
    "\n",
    "\n",
    "        if self.cfg.get(\"sparsity_type\", \"l1\") == \"l1\":\n",
    "            # Calculate L1 loss if configured (original method)\n",
    "            with torch.no_grad():\n",
    "                decoder_norms = self.W_dec.norm(dim=-1)\n",
    "                total_decoder_norm = einops.reduce(decoder_norms, 'h n -> h', 'sum')\n",
    "            l1_loss_val = (acts.float() * total_decoder_norm[None, :]).sum(dim=-1).mean()\n",
    "        elif self.cfg.get(\"sparsity_type\", \"l1\") == \"batch_top_k\":\n",
    "            # For BatchTopK, the primary loss is L2.\n",
    "            pass # No direct L1 term. Sparsity is structural.\n",
    "\n",
    "\n",
    "        # --- Explained Variance ---\n",
    "        with torch.no_grad():\n",
    "            variance = einops.reduce((x - x.mean(dim=0, keepdim=True)).pow(2), 'b n d -> b', 'sum')\n",
    "            explained_variance = (1 - l2_per_batch / (variance + 1e-8)).mean()\n",
    "            variance_A = (x[:, 0] - x[:, 0].mean(dim=0, keepdim=True)).pow(2).sum(dim=-1)\n",
    "            l2_per_batch_A = squared_diff[:, 0].sum(dim=-1)\n",
    "            explained_variance_A = (1 - l2_per_batch_A / (variance_A + 1e-8)).mean()\n",
    "            variance_B = (x[:, 1] - x[:, 1].mean(dim=0, keepdim=True)).pow(2).sum(dim=-1)\n",
    "            l2_per_batch_B = squared_diff[:, 1].sum(dim=-1)\n",
    "            explained_variance_B = (1 - l2_per_batch_B / (variance_B + 1e-8)).mean()\n",
    "\n",
    "\n",
    "        return LossOutput(l2_loss, l1_loss_val, l0_loss_val, explained_variance, explained_variance_A, explained_variance_B)\n",
    "\n",
    "    def create_save_dir(self, base_dir_str=\"./checkpoints\"):\n",
    "        base_dir = Path(base_dir_str)\n",
    "        base_dir.mkdir(parents=True, exist_ok=True) # Ensure base directory exists\n",
    "\n",
    "\n",
    "        # Find existing version directories\n",
    "        version_list = []\n",
    "        for file in base_dir.iterdir():\n",
    "            if file.is_dir() and file.name.startswith(\"version_\"):\n",
    "                try:\n",
    "                    version_list.append(int(file.name.split(\"_\")[1]))\n",
    "                except (IndexError, ValueError):\n",
    "                    continue # Ignore directories not matching the pattern\n",
    "\n",
    "\n",
    "        # Determine the next version number\n",
    "        if version_list:\n",
    "            version = 1 + max(version_list)\n",
    "        else:\n",
    "            version = 0\n",
    "\n",
    "\n",
    "        self.save_dir = base_dir / f\"version_{version}\"\n",
    "        self.save_dir.mkdir(parents=True)\n",
    "        print(f\"Created checkpoint directory: {self.save_dir}\")\n",
    "\n",
    "    def save(self, checkpoint_dir_str=\"./checkpoints\"):\n",
    "        \"\"\"Saves the model state dictionary and config.\"\"\"\n",
    "        if self.save_dir is None:\n",
    "            self.create_save_dir(checkpoint_dir_str)\n",
    "\n",
    "\n",
    "        # Define file paths within the versioned directory\n",
    "        weight_path = self.save_dir / f\"crosscoder_{self.save_version}.pt\"\n",
    "        cfg_path = self.save_dir / f\"crosscoder_{self.save_version}_cfg.json\"\n",
    "\n",
    "\n",
    "        # Save the model's learned parameters\n",
    "        torch.save(self.state_dict(), weight_path)\n",
    "\n",
    "\n",
    "        # Save the configuration used for this model\n",
    "        with open(cfg_path, \"w\") as f:\n",
    "            # Convert Path objects in config to strings for JSON serialization\n",
    "            serializable_cfg = {k: str(v) if isinstance(v, Path) else v for k, v in self.cfg.items()}\n",
    "            json.dump(serializable_cfg, f, indent=2)\n",
    "\n",
    "\n",
    "        print(f\"Saved checkpoint {self.save_version} to {self.save_dir}\")\n",
    "        self.save_version += 1 # Increment version for the next save\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, version_dir_str: str, checkpoint_version: int = 0):\n",
    "        \"\"\"Loads a CrossCoder model from a saved checkpoint directory.\"\"\"\n",
    "        save_dir = Path(version_dir_str)\n",
    "        cfg_path = save_dir / f\"crosscoder_{checkpoint_version}_cfg.json\"\n",
    "        weight_path = save_dir / f\"crosscoder_{checkpoint_version}.pt\"\n",
    "\n",
    "\n",
    "        if not cfg_path.exists() or not weight_path.exists():\n",
    "                raise FileNotFoundError(f\"Checkpoint files not found in {save_dir} for version {checkpoint_version}\")\n",
    "\n",
    "\n",
    "        print(f\"Loading config from: {cfg_path}\")\n",
    "        with open(cfg_path, \"r\") as f:\n",
    "            cfg = json.load(f)\n",
    "        print(\"Loaded Config:\")\n",
    "        pprint.pprint(cfg)\n",
    "\n",
    "\n",
    "        # Create a new instance with the loaded config\n",
    "        # Ensure device is handled correctly if loading on different hardware\n",
    "        if 'device' not in cfg:\n",
    "                cfg['device'] = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "                print(f\"Warning: 'device' not found in config, defaulting to {cfg['device']}\")\n",
    "        instance = cls(cfg=cfg)\n",
    "\n",
    "\n",
    "        print(f\"Loading weights from: {weight_path}\")\n",
    "        # Load the saved weights onto the correct device specified in the config\n",
    "        state_dict = torch.load(weight_path, map_location=cfg[\"device\"])\n",
    "        instance.load_state_dict(state_dict)\n",
    "        print(\"Weights loaded successfully.\")\n",
    "\n",
    "\n",
    "        instance.save_dir = save_dir # Set save_dir for potential future saves\n",
    "        instance.save_version = checkpoint_version + 1 # Start saving from next version\n",
    "\n",
    "\n",
    "        return instance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
